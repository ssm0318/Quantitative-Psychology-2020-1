{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNVM(text):\n",
    "    tokenizer = MeCab.Tagger()\n",
    "    parsed = tokenizer.parse(text)\n",
    "    word_tag = [w for w in parsed.split(\"\\n\")]\n",
    "    pos = []\n",
    "    tags = ['NNG','NNP','VV','VA','VCP','VCN']\n",
    "    for word_ in word_tag[:-2]:\n",
    "        word = word_.split(\"\\t\")\n",
    "        tag = word[1].split(\",\")[0]\n",
    "        if (tag in tags):\n",
    "            pos.append(word[0])\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNVM_lemma(text):\n",
    "    tokenizer = MeCab.Tagger()\n",
    "    parsed = tokenizer.parse(text)\n",
    "    word_tag = [w for w in parsed.split(\"\\n\")]\n",
    "    pos = []\n",
    "    tags = ['NNG','NNP','VV','VA', 'VX', 'VCP','VCN']\n",
    "    for word_ in word_tag[:-2]:\n",
    "        word = word_.split(\"\\t\")\n",
    "        tag = word[1].split(\",\")\n",
    "        if (tag[0] in tags):\n",
    "            pos.append(word[0])\n",
    "        elif('+' in tag[0]):\n",
    "            if('VV' in tag[0] or 'VA' in tag[0] or 'VX' in tag[0]):\n",
    "                t = tag[-1].split('/')[0]\n",
    "                pos.append(t)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['거우', '날', '내리', '비', '빨래', '소풍', '오', '오늘', '있', '전']\n",
      "  (0, 7)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 8)\t1\n",
      "['거우', '날', '내리', '비', '빨래', '소풍', '오', '오늘', '있', '전']\n",
      "  (0, 0)\t0.4323850887896905\n",
      "  (0, 4)\t0.4323850887896905\n",
      "  (0, 9)\t0.4323850887896905\n",
      "  (0, 6)\t0.4323850887896905\n",
      "  (0, 3)\t0.25537359879528915\n",
      "  (0, 7)\t0.4323850887896905\n",
      "  (1, 1)\t0.7203334490549893\n",
      "  (1, 2)\t0.5478321549274363\n",
      "  (1, 3)\t0.4254405389711991\n",
      "  (2, 8)\t0.5844829010200651\n",
      "  (2, 5)\t0.5844829010200651\n",
      "  (2, 2)\t0.444514311537431\n",
      "  (2, 3)\t0.34520501686496574\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "#     text = \"아버지가방에들어가신다\"\n",
    "#     s = \"우리는 가까워질 수 없기 때문에 가깝게 느껴지지 않는다\"\n",
    "#     print(getNVM(text))\n",
    "    docs = ['오늘은 비가 오기 전에 빨래를 거우어야 한다.',\n",
    "           '비가 내리는 어느 날에는 네가 생각나.',\n",
    "           '오늘 비가 내리지 않으면 소풍을 갈 수 있어']\n",
    "    tf_vect = CountVectorizer(tokenizer=getNVM, preprocessor=None, lowercase=False)\n",
    "    dtm = tf_vect.fit_transform(docs)\n",
    "    print(tf_vect.get_feature_names())\n",
    "    print(dtm)\n",
    "    \n",
    "    tfidf_vect = TfidfVectorizer(tokenizer=getNVM, preprocessor=None, lowercase=False)\n",
    "    dtm = tfidf_vect.fit_transform(docs)\n",
    "    print(tfidf_vect.get_feature_names())\n",
    "    print(dtm)\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
